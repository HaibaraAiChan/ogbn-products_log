Using backend: pytorch
main start at this time 1648254378.5925252
#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
196571
39255
2164782
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

in feats:  100
----------------------------------------before generate_dataloader_block 
 Nvidia-smi: 2.0242919921875 GB
    Memory Allocated: 0.0006527900695800781  GigaBytes
Max Memory Allocated: 0.0006527900695800781  GigaBytes

The real block id is  2
get_global_graph_edges_ids_block function  spend 0.4222888946533203
range selection method range initialization spend 0.02963542938232422
time for parepare:  0.29628443717956543
local_output_nid generation:  0.026084184646606445
local_in_edges_tensor generation:  0.08904886245727539
mini_batch_src_global generation:  0.1255662441253662
r_  generation:  2.2046563625335693
local_output_nid generation:  0.0295867919921875
local_in_edges_tensor generation:  0.07952260971069336
mini_batch_src_global generation:  0.19021892547607422
r_  generation:  2.1233558654785156
----------------------check_connections_block total spend ----------------------------- 5.892254114151001
generate_one_block  8.52607274055481
generate_one_block  2.737260341644287
The real block id is  1
get_global_graph_edges_ids_block function  spend 2.076251983642578
gen group dst list time:  0.05936479568481445
time for parepare:  0.4159064292907715
local_output_nid generation:  0.18654918670654297
local_in_edges_tensor generation:  0.7254307270050049
mini_batch_src_global generation:  1.0577664375305176
r_  generation:  14.713687181472778
local_output_nid generation:  0.34508395195007324
local_in_edges_tensor generation:  0.9634377956390381
mini_batch_src_global generation:  1.3362171649932861
r_  generation:  14.719891786575317
----------------------check_connections_block total spend ----------------------------- 39.37755537033081
generate_one_block  19.028273344039917
generate_one_block  19.119879245758057
The real block id is  0
get_global_graph_edges_ids_block function  spend 2.1709911823272705
gen group dst list time:  0.15124177932739258
time for parepare:  0.4718465805053711
local_output_nid generation:  0.37975025177001953
local_in_edges_tensor generation:  1.1873393058776855
mini_batch_src_global generation:  1.3036019802093506
r_  generation:  19.82693648338318
local_output_nid generation:  0.654911994934082
local_in_edges_tensor generation:  1.5373449325561523
mini_batch_src_global generation:  1.6346335411071777
r_  generation:  18.090954065322876
----------------------check_connections_block total spend ----------------------------- 51.65793776512146
generate_one_block  23.62574005126953
generate_one_block  23.18330717086792
-----------------------------------------after block dataloader generation 
 Nvidia-smi: 2.0242919921875 GB
    Memory Allocated: 0.0006527900695800781  GigaBytes
Max Memory Allocated: 0.0006527900695800781  GigaBytes

connection checking time:  91.03549313545227
block generation total time  84.95719981193542
average batch blocks generation time:  42.47859990596771
----------------------------------------before batch_pred = model(blocks, batch_inputs) 
 Nvidia-smi: 3.5008544921875 GB
    Memory Allocated: 1.4099292755126953  GigaBytes
Max Memory Allocated: 1.4099292755126953  GigaBytes

Traceback (most recent call last):
  File "pseudo_mini_batch_range_products_sage.py", line 455, in <module>
    main()
  File "pseudo_mini_batch_range_products_sage.py", line 451, in main
    best_test = run(args, device, data)
  File "pseudo_mini_batch_range_products_sage.py", line 276, in run
    batch_pred = model(blocks, batch_inputs)#------------*
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/graph_partition_multi_layers/pseudo_mini_batch_full_batch/SAGE/graphsage_model_products.py", line 48, in forward
    x = layer(block, x)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 258, in forward
    graph.update_all(msg_fn, self._lstm_reducer)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/heterograph.py", line 4849, in update_all
    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 337, in message_passing
    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/core.py", line 143, in invoke_udf_reduce
    bkt_rsts.append(func(nbatch))
  File "/home/cc/.local/lib/python3.6/site-packages/dgl/nn/pytorch/conv/sageconv.py", line 173, in _lstm_reducer
    _, (rst, _) = self.lstm(m, h)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/cc/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 23.62 GiB total capacity; 20.77 GiB already allocated; 67.44 MiB free; 20.92 GiB reserved in total by PyTorch)
